**************************************************
(A) Model: (A) Base-DT

--------------------------------------------------

(B) Confusion Matrix:
[[36  0  1]
 [ 1 16  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.97      0.97      0.97        37
   Chinstrap       1.00      0.94      0.97        17
      Gentoo       0.97      1.00      0.98        30

    accuracy                           0.98        84
   macro avg       0.98      0.97      0.98        84
weighted avg       0.98      0.98      0.98        84


--------------------------------------------------

(D) Accuracy: 0.98
Macro-average F1: 0.98
Weighted-average F1: 0.98

--------------------------------------------------

**************************************************
(A) Model: (B) TOP-DT

Parameters:
criterion: ['gini', 'entropy']
max_depth: [3, 5, None]
min_samples_split: [2, 5, 10]

Best Parameters:
criterion: entropy
max_depth: 5
min_samples_split: 2

--------------------------------------------------

(B) Confusion Matrix:
[[36  0  1]
 [ 2 15  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.95      0.97      0.96        37
   Chinstrap       1.00      0.88      0.94        17
      Gentoo       0.97      1.00      0.98        30

    accuracy                           0.96        84
   macro avg       0.97      0.95      0.96        84
weighted avg       0.97      0.96      0.96        84


--------------------------------------------------

(D) Accuracy: 0.96
Macro-average F1: 0.96
Weighted-average F1: 0.96

--------------------------------------------------

**************************************************
(A) Model: (C) Base-MLP

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [17  0  0]
 [30  0  0]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.44      1.00      0.61        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.00      0.00      0.00        30

    accuracy                           0.44        84
   macro avg       0.15      0.33      0.20        84
weighted avg       0.19      0.44      0.27        84


--------------------------------------------------

(D) Accuracy: 0.44
Macro-average F1: 0.20
Weighted-average F1: 0.27

--------------------------------------------------

**************************************************
(A) Model: (D) TOP-MLP

Parameters:
activation: ['logistic', 'tanh', 'relu']
hidden_layer_sizes: [(30, 50), (10, 10, 10)]
solver: ['adam', 'sgd']

Best Parameters:
activation: tanh
hidden_layer_sizes: (30, 50)
solver: adam

--------------------------------------------------

(B) Confusion Matrix:
[[ 0  0 37]
 [ 0  0 17]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.00      0.00      0.00        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.36      1.00      0.53        30

    accuracy                           0.36        84
   macro avg       0.12      0.33      0.18        84
weighted avg       0.13      0.36      0.19        84


--------------------------------------------------

(D) Accuracy: 0.36
Macro-average F1: 0.18
Weighted-average F1: 0.19

--------------------------------------------------





*-*-*-*-*-*-*-*-*-*-*-* RUNNING STEP 6 *-*-*-*-*-*-*-*-*-*-*-*
**************************************************
(A) Model: (A) Base-DT

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [ 1 16  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.97      1.00      0.99        37
   Chinstrap       1.00      0.94      0.97        17
      Gentoo       1.00      1.00      1.00        30

    accuracy                           0.99        84
   macro avg       0.99      0.98      0.99        84
weighted avg       0.99      0.99      0.99        84


--------------------------------------------------

(D) Accuracy: 0.99
Macro-average F1: 0.99
Weighted-average F1: 0.99

--------------------------------------------------

**************************************************
(A) Model: (B) TOP-DT

Parameters:
criterion: ['gini', 'entropy']
max_depth: [3, 5, None]
min_samples_split: [2, 5, 10]

Best Parameters:
criterion: gini
max_depth: 5
min_samples_split: 2

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [ 1 16  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.97      1.00      0.99        37
   Chinstrap       1.00      0.94      0.97        17
      Gentoo       1.00      1.00      1.00        30

    accuracy                           0.99        84
   macro avg       0.99      0.98      0.99        84
weighted avg       0.99      0.99      0.99        84


--------------------------------------------------

(D) Accuracy: 0.99
Macro-average F1: 0.99
Weighted-average F1: 0.99

--------------------------------------------------

**************************************************
(A) Model: (C) Base-MLP

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [17  0  0]
 [30  0  0]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.44      1.00      0.61        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.00      0.00      0.00        30

    accuracy                           0.44        84
   macro avg       0.15      0.33      0.20        84
weighted avg       0.19      0.44      0.27        84


--------------------------------------------------

(D) Accuracy: 0.44
Macro-average F1: 0.20
Weighted-average F1: 0.27

--------------------------------------------------

**************************************************
(A) Model: (D) TOP-MLP

Parameters:
activation: ['logistic', 'tanh', 'relu']
hidden_layer_sizes: [(30, 50), (10, 10, 10)]
solver: ['adam', 'sgd']

Best Parameters:
activation: logistic
hidden_layer_sizes: (30, 50)
solver: adam

--------------------------------------------------

(B) Confusion Matrix:
[[ 8 27  2]
 [ 2 15  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.80      0.22      0.34        37
   Chinstrap       0.36      0.88      0.51        17
      Gentoo       0.94      1.00      0.97        30

    accuracy                           0.63        84
   macro avg       0.70      0.70      0.61        84
weighted avg       0.76      0.63      0.60        84


--------------------------------------------------

(D) Accuracy: 0.63
Macro-average F1: 0.61
Weighted-average F1: 0.60

--------------------------------------------------

**************************************************
(A) Model: (A) Base-DT

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [ 1 16  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.97      1.00      0.99        37
   Chinstrap       1.00      0.94      0.97        17
      Gentoo       1.00      1.00      1.00        30

    accuracy                           0.99        84
   macro avg       0.99      0.98      0.99        84
weighted avg       0.99      0.99      0.99        84


--------------------------------------------------

(D) Accuracy: 0.99
Macro-average F1: 0.99
Weighted-average F1: 0.99

--------------------------------------------------

**************************************************
(A) Model: (B) TOP-DT

Parameters:
criterion: ['gini', 'entropy']
max_depth: [3, 5, None]
min_samples_split: [2, 5, 10]

Best Parameters:
criterion: gini
max_depth: 5
min_samples_split: 2

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [ 2 15  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.95      1.00      0.97        37
   Chinstrap       1.00      0.88      0.94        17
      Gentoo       1.00      1.00      1.00        30

    accuracy                           0.98        84
   macro avg       0.98      0.96      0.97        84
weighted avg       0.98      0.98      0.98        84


--------------------------------------------------

(D) Accuracy: 0.98
Macro-average F1: 0.97
Weighted-average F1: 0.98

--------------------------------------------------

**************************************************
(A) Model: (C) Base-MLP

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [17  0  0]
 [30  0  0]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.44      1.00      0.61        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.00      0.00      0.00        30

    accuracy                           0.44        84
   macro avg       0.15      0.33      0.20        84
weighted avg       0.19      0.44      0.27        84


--------------------------------------------------

(D) Accuracy: 0.44
Macro-average F1: 0.20
Weighted-average F1: 0.27

--------------------------------------------------

**************************************************
(A) Model: (D) TOP-MLP

Parameters:
activation: ['logistic', 'tanh', 'relu']
hidden_layer_sizes: [(30, 50), (10, 10, 10)]
solver: ['adam', 'sgd']

Best Parameters:
activation: relu
hidden_layer_sizes: (10, 10, 10)
solver: adam

--------------------------------------------------

(B) Confusion Matrix:
[[29  2  6]
 [ 0 17  0]
 [ 0  6 24]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       1.00      0.78      0.88        37
   Chinstrap       0.68      1.00      0.81        17
      Gentoo       0.80      0.80      0.80        30

    accuracy                           0.83        84
   macro avg       0.83      0.86      0.83        84
weighted avg       0.86      0.83      0.84        84


--------------------------------------------------

(D) Accuracy: 0.83
Macro-average F1: 0.83
Weighted-average F1: 0.84

--------------------------------------------------

**************************************************
(A) Model: (A) Base-DT

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [ 2 15  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.95      1.00      0.97        37
   Chinstrap       1.00      0.88      0.94        17
      Gentoo       1.00      1.00      1.00        30

    accuracy                           0.98        84
   macro avg       0.98      0.96      0.97        84
weighted avg       0.98      0.98      0.98        84


--------------------------------------------------

(D) Accuracy: 0.98
Macro-average F1: 0.97
Weighted-average F1: 0.98

--------------------------------------------------

**************************************************
(A) Model: (B) TOP-DT

Parameters:
criterion: ['gini', 'entropy']
max_depth: [3, 5, None]
min_samples_split: [2, 5, 10]

Best Parameters:
criterion: gini
max_depth: None
min_samples_split: 2

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [ 1 16  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.97      1.00      0.99        37
   Chinstrap       1.00      0.94      0.97        17
      Gentoo       1.00      1.00      1.00        30

    accuracy                           0.99        84
   macro avg       0.99      0.98      0.99        84
weighted avg       0.99      0.99      0.99        84


--------------------------------------------------

(D) Accuracy: 0.99
Macro-average F1: 0.99
Weighted-average F1: 0.99

--------------------------------------------------

**************************************************
(A) Model: (C) Base-MLP

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [17  0  0]
 [30  0  0]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.44      1.00      0.61        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.00      0.00      0.00        30

    accuracy                           0.44        84
   macro avg       0.15      0.33      0.20        84
weighted avg       0.19      0.44      0.27        84


--------------------------------------------------

(D) Accuracy: 0.44
Macro-average F1: 0.20
Weighted-average F1: 0.27

--------------------------------------------------

**************************************************
(A) Model: (D) TOP-MLP

Parameters:
activation: ['logistic', 'tanh', 'relu']
hidden_layer_sizes: [(30, 50), (10, 10, 10)]
solver: ['adam', 'sgd']

Best Parameters:
activation: relu
hidden_layer_sizes: (10, 10, 10)
solver: adam

--------------------------------------------------

(B) Confusion Matrix:
[[17  0 20]
 [14  0  3]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.55      0.46      0.50        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.57      1.00      0.72        30

    accuracy                           0.56        84
   macro avg       0.37      0.49      0.41        84
weighted avg       0.44      0.56      0.48        84


--------------------------------------------------

(D) Accuracy: 0.56
Macro-average F1: 0.41
Weighted-average F1: 0.48

--------------------------------------------------

**************************************************
(A) Model: (A) Base-DT

--------------------------------------------------

(B) Confusion Matrix:
[[36  0  1]
 [ 2 15  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.95      0.97      0.96        37
   Chinstrap       1.00      0.88      0.94        17
      Gentoo       0.97      1.00      0.98        30

    accuracy                           0.96        84
   macro avg       0.97      0.95      0.96        84
weighted avg       0.97      0.96      0.96        84


--------------------------------------------------

(D) Accuracy: 0.96
Macro-average F1: 0.96
Weighted-average F1: 0.96

--------------------------------------------------

**************************************************
(A) Model: (B) TOP-DT

Parameters:
criterion: ['gini', 'entropy']
max_depth: [3, 5, None]
min_samples_split: [2, 5, 10]

Best Parameters:
criterion: gini
max_depth: 5
min_samples_split: 2

--------------------------------------------------

(B) Confusion Matrix:
[[36  0  1]
 [ 2 15  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.95      0.97      0.96        37
   Chinstrap       1.00      0.88      0.94        17
      Gentoo       0.97      1.00      0.98        30

    accuracy                           0.96        84
   macro avg       0.97      0.95      0.96        84
weighted avg       0.97      0.96      0.96        84


--------------------------------------------------

(D) Accuracy: 0.96
Macro-average F1: 0.96
Weighted-average F1: 0.96

--------------------------------------------------

**************************************************
(A) Model: (C) Base-MLP

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [17  0  0]
 [30  0  0]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.44      1.00      0.61        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.00      0.00      0.00        30

    accuracy                           0.44        84
   macro avg       0.15      0.33      0.20        84
weighted avg       0.19      0.44      0.27        84


--------------------------------------------------

(D) Accuracy: 0.44
Macro-average F1: 0.20
Weighted-average F1: 0.27

--------------------------------------------------

**************************************************
(A) Model: (D) TOP-MLP

Parameters:
activation: ['logistic', 'tanh', 'relu']
hidden_layer_sizes: [(30, 50), (10, 10, 10)]
solver: ['adam', 'sgd']

Best Parameters:
activation: tanh
hidden_layer_sizes: (30, 50)
solver: adam

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [17  0  0]
 [30  0  0]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.44      1.00      0.61        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.00      0.00      0.00        30

    accuracy                           0.44        84
   macro avg       0.15      0.33      0.20        84
weighted avg       0.19      0.44      0.27        84


--------------------------------------------------

(D) Accuracy: 0.44
Macro-average F1: 0.20
Weighted-average F1: 0.27

--------------------------------------------------

**************************************************
(A) Model: (A) Base-DT

--------------------------------------------------

(B) Confusion Matrix:
[[36  0  1]
 [ 1 16  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.97      0.97      0.97        37
   Chinstrap       1.00      0.94      0.97        17
      Gentoo       0.97      1.00      0.98        30

    accuracy                           0.98        84
   macro avg       0.98      0.97      0.98        84
weighted avg       0.98      0.98      0.98        84


--------------------------------------------------

(D) Accuracy: 0.98
Macro-average F1: 0.98
Weighted-average F1: 0.98

--------------------------------------------------

**************************************************
(A) Model: (B) TOP-DT

Parameters:
criterion: ['gini', 'entropy']
max_depth: [3, 5, None]
min_samples_split: [2, 5, 10]

Best Parameters:
criterion: gini
max_depth: None
min_samples_split: 2

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [ 1 16  0]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.97      1.00      0.99        37
   Chinstrap       1.00      0.94      0.97        17
      Gentoo       1.00      1.00      1.00        30

    accuracy                           0.99        84
   macro avg       0.99      0.98      0.99        84
weighted avg       0.99      0.99      0.99        84


--------------------------------------------------

(D) Accuracy: 0.99
Macro-average F1: 0.99
Weighted-average F1: 0.99

--------------------------------------------------

**************************************************
(A) Model: (C) Base-MLP

--------------------------------------------------

(B) Confusion Matrix:
[[37  0  0]
 [17  0  0]
 [30  0  0]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.44      1.00      0.61        37
   Chinstrap       0.00      0.00      0.00        17
      Gentoo       0.00      0.00      0.00        30

    accuracy                           0.44        84
   macro avg       0.15      0.33      0.20        84
weighted avg       0.19      0.44      0.27        84


--------------------------------------------------

(D) Accuracy: 0.44
Macro-average F1: 0.20
Weighted-average F1: 0.27

--------------------------------------------------

**************************************************
(A) Model: (D) TOP-MLP

Parameters:
activation: ['logistic', 'tanh', 'relu']
hidden_layer_sizes: [(30, 50), (10, 10, 10)]
solver: ['adam', 'sgd']

Best Parameters:
activation: relu
hidden_layer_sizes: (30, 50)
solver: adam

--------------------------------------------------

(B) Confusion Matrix:
[[ 0 23 14]
 [ 0 14  3]
 [ 0  0 30]]

--------------------------------------------------

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.00      0.00      0.00        37
   Chinstrap       0.38      0.82      0.52        17
      Gentoo       0.64      1.00      0.78        30

    accuracy                           0.52        84
   macro avg       0.34      0.61      0.43        84
weighted avg       0.30      0.52      0.38        84


--------------------------------------------------

(D) Accuracy: 0.52
Macro-average F1: 0.43
Weighted-average F1: 0.38

--------------------------------------------------

--------------------------------------------------
The 5 runs for Base DT resulted in
accuracy: [0.99, 0.99, 0.98, 0.96, 0.98]
f1_macro: [0.99, 0.99, 0.97, 0.96, 0.98]
f1_weighted [0.99, 0.99, 0.98, 0.96, 0.98]

Average Accuracy (Base DT): 0.98, Variance: 7.94e-05
Average Macro F1 (Base DT): 0.98, Variance: 9.06e-05
Average Weighted F1 (Base DT): 0.98, Variance: 8.14e-05

--------------------------------------------------
The 5 runs for Top DT resulted in
accuracy: [0.99, 0.98, 0.99, 0.96, 0.99]
f1_macro: [0.99, 0.97, 0.99, 0.96, 0.99]
f1_weighted [0.99, 0.98, 0.99, 0.96, 0.99]

Average Accuracy (Top DT): 0.98, Variance: 9.07e-05
Average Macro F1 (Top DT): 0.98, Variance: 1.07e-04
Average Weighted F1 (Top DT): 0.98, Variance: 9.34e-05

--------------------------------------------------

The 5 runs for Base MLP resulted in
accuracy: [0.44, 0.44, 0.44, 0.44, 0.44]
f1_macro: [0.2, 0.2, 0.2, 0.2, 0.2]
f1_weighted [0.27, 0.27, 0.27, 0.27, 0.27]

Average Accuracy (Base MLP): 0.44, Variance: 3.08e-33
Average Macro F1 (Base MLP): 0.20, Variance: 0.00e+00
Average Weighted F1 (Base MLP): 0.27, Variance: 0.00e+00

--------------------------------------------------

The 5 runs for Top MLP resulted in
accuracy: [0.63, 0.83, 0.56, 0.44, 0.52]
f1_macro: [0.61, 0.83, 0.41, 0.2, 0.43]
f1_weighted [0.6, 0.84, 0.48, 0.27, 0.38]

Average Accuracy (Base MLP): 0.60, Variance: 1.77e-02
Average Macro F1 (Base MLP): 0.60, Variance: 1.77e-02
Average Weighted F1 (Base MLP): 0.60, Variance: 1.77e-02

