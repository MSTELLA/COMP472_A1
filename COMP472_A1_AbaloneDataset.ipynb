{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:43:48.071014200Z",
     "start_time": "2023-11-11T06:43:48.059015100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Team Bean Burrito\n",
    "'''\n",
    "Gabrielle Guidote 40175182\n",
    "Marie-Jose Castellanos 40168044\n",
    "Amrit Sohpal 40176197\n",
    "'''\n",
    "# Assignment 1\n",
    "# Abalone Dataset\n",
    "output_file = \"abalone-performance.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:43:48.087150300Z",
     "start_time": "2023-11-11T06:43:48.073147800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "#5\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#4 a : DT\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "#4 b : DT GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#4 c: MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# performance metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def write_performance_to_file(filename, model_description, confusion_mtrx, classification, accuracy, macro_avg_f1, weight_avg_f1):\n",
    "    # 'a' opens a file, if file doesn't exist, create a new file. \n",
    "    with open(filename, 'a') as file:\n",
    "        # Separator and Model Description\n",
    "        file.write(\"--------------------------------------------------\\n\")\n",
    "        file.write(f\"Model: {model_description}\\n\\n\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        file.write(\"(B) Confusion Matrix:\\n\")\n",
    "        file.write(f\"{confusion_mtrx}\\n\\n\")\n",
    "\n",
    "        # Classification Report (Precision, Recall, F1-measure for each class)\n",
    "        file.write(\"(C) Classification Report:\\n\")\n",
    "        file.write(f\"{classification}\\n\\n\")\n",
    "\n",
    "        # Accuracy, Macro-average F1 and Weighted-average F1\n",
    "        file.write(\"(D) Accuracy: {:.2f}\\n\".format(accuracy))\n",
    "        file.write(\"Macro-average F1: {:.2f}\\n\".format(macro_avg_f1))\n",
    "        file.write(\"Weighted-average F1: {:.2f}\\n\".format(weight_avg_f1))\n",
    "        file.write(\"--------------------------------------------------\\n\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T06:43:48.124175Z",
     "start_time": "2023-11-11T06:43:48.089147100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET 1 :  ABALONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:43:48.149206Z",
     "start_time": "2023-11-11T06:43:48.103662500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4177 entries, 0 to 4176\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Type           4177 non-null   object \n",
      " 1   LongestShell   4177 non-null   float64\n",
      " 2   Diameter       4177 non-null   float64\n",
      " 3   Height         4177 non-null   float64\n",
      " 4   WholeWeight    4177 non-null   float64\n",
      " 5   ShuckedWeight  4177 non-null   float64\n",
      " 6   VisceraWeight  4177 non-null   float64\n",
      " 7   ShellWeight    4177 non-null   float64\n",
      " 8   Rings          4177 non-null   int64  \n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 293.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  Type  LongestShell  Diameter  Height  WholeWeight  ShuckedWeight  \\\n0    F         0.605     0.470   0.165       1.1775         0.6110   \n1    M         0.550     0.425   0.150       0.8315         0.4110   \n2    M         0.460     0.345   0.110       0.4595         0.2350   \n3    F         0.650     0.475   0.165       1.3875         0.5800   \n4    M         0.575     0.470   0.140       0.8375         0.3485   \n\n   VisceraWeight  ShellWeight  Rings  \n0         0.2275       0.2920      9  \n1         0.1765       0.2165     10  \n2         0.0885       0.1160      7  \n3         0.3485       0.3095      9  \n4         0.1735       0.2400     11  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>LongestShell</th>\n      <th>Diameter</th>\n      <th>Height</th>\n      <th>WholeWeight</th>\n      <th>ShuckedWeight</th>\n      <th>VisceraWeight</th>\n      <th>ShellWeight</th>\n      <th>Rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>F</td>\n      <td>0.605</td>\n      <td>0.470</td>\n      <td>0.165</td>\n      <td>1.1775</td>\n      <td>0.6110</td>\n      <td>0.2275</td>\n      <td>0.2920</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>M</td>\n      <td>0.550</td>\n      <td>0.425</td>\n      <td>0.150</td>\n      <td>0.8315</td>\n      <td>0.4110</td>\n      <td>0.1765</td>\n      <td>0.2165</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M</td>\n      <td>0.460</td>\n      <td>0.345</td>\n      <td>0.110</td>\n      <td>0.4595</td>\n      <td>0.2350</td>\n      <td>0.0885</td>\n      <td>0.1160</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>F</td>\n      <td>0.650</td>\n      <td>0.475</td>\n      <td>0.165</td>\n      <td>1.3875</td>\n      <td>0.5800</td>\n      <td>0.3485</td>\n      <td>0.3095</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>M</td>\n      <td>0.575</td>\n      <td>0.470</td>\n      <td>0.140</td>\n      <td>0.8375</td>\n      <td>0.3485</td>\n      <td>0.1735</td>\n      <td>0.2400</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data regarding Abalones.contains features of physical description of abalones (length, diameter, weights, etc) and one of 3 possible values for their\n",
    "# sex: M (male), F (female), I (infant). Given the physical features of the abalone, the goal is to predict their sex.\n",
    "df = pd.read_csv(\"abalone.csv\")\n",
    "df.info()\n",
    "df.head()\n",
    "# All features are numerical. Only the classes are string. No changes necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the percentage of the instances in each output class and store the graphic in a file called penguin-classes.gif\n",
    "/ abalone-classes.gif. This analysis of the dataset will allow you to determine if the classes are balanced.\n",
    "Which metric is more appropriate to use to evaluate the performance. Be ready to discuss this at the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-11T06:43:48.132696300Z"
    }
   },
   "outputs": [],
   "source": [
    "class_count = df['Type'].value_counts()\n",
    "plt.pie(class_count, labels=class_count.index, autopct='%1.1f%%') # TODO : CAHNGE TYPE?? COLOR? HARD TO SEE SAVED IMAGE\n",
    "plt.title('Distribution of Type')\n",
    "# plt.show()\n",
    "\n",
    "# save image (she said not .gif is ok)\n",
    "plt.savefig('abalone-classes.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the dataset using train test split using the default parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "X =  df.loc[:, df.columns != 'Type']\n",
    "y = df['Type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train and test 4 different classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base DT\n",
    "(a) Base-DT: a Decision Tree with the default parameters. Show the decision tree graphically (for the\n",
    "abalone dataset, you can restrict the tree depth for visualisation purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Training base DT\n",
    "base_DT_d2 = DecisionTreeClassifier()\n",
    "base_DT_d2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Tree Visualization : https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html TODO : change figure size because its really tiny\n",
    "tree.plot_tree(base_DT_d2, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30 , 20))  # Set the figure size (width, height) in inches\n",
    "plot_tree(base_DT_d2, filled=True, feature_names=X.columns, class_names=np.unique(y_train), max_depth=5)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Testing base DT\n",
    "predictions2 = base_DT_d2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "print(confusion_matrix(y_test, predictions2))\n",
    "\n",
    "print(classification_report(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Output file\n",
    "f1_scr_weighted = f1_score(y_test,predictions2, average=\"weighted\")\n",
    "f1_scr_macro = f1_score(y_test,predictions2, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, predictions2)\n",
    "\n",
    "write_performance_to_file(output_file, \"(A) Base-DT\", confusion_matrix(y_test, predictions2), classification_report(y_test, predictions2), accuracy, f1_scr_macro, f1_scr_weighted)\n",
    "# (A) a clear separator (a sequence of hyphens or stars) and a string clearly describing the model \n",
    "    # (e.g. the model name + hyper-parameter values that you changed). In the case of Top-DT and Top-MLP,\n",
    "    # display the best hyperparameters found by the gridsearch.\n",
    "\n",
    "# (B) the confusion matrix\n",
    "\n",
    "# (C) the precision, recall, and F1-measure for each class\n",
    "\n",
    "# (D) the accuracy, macro-average F1 and weighted-average F1 of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOP DT\n",
    "(b) Top-DT: a better performing Decision Tree found using a gridsearch. The gridsearch will allow you\n",
    "to find the best combination of hyper-parameters, as determined by the evaluation function that you\n",
    "have determined in step (3) above. The hyper-parameters that you will experiment with are:\n",
    "• criterion: gini or entropy\n",
    "• max depth : 2 different values of your choice and ”None”\n",
    "• min samples split: 3 different values of your choice\n",
    "Show the decision tree graphically (for the abalone dataset, you can restrict the tree depth for visualisation purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Training top DT\n",
    "parameters = {'criterion':['gini','entropy'], 'max_depth':[3,10,None], 'min_samples_split':[2,5,10]}\n",
    "\n",
    "top_DT_d2 = DecisionTreeClassifier()\n",
    "top_DT_d2 = GridSearchCV(top_DT_d2, parameters)\n",
    "\n",
    "top_DT_d2.fit(X_train, y_train)\n",
    "\n",
    "print(top_DT_d2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Testing base DT\n",
    "predictions = top_DT_d2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Output file\n",
    "f1_scr_weighted = f1_score(y_test,predictions, average=\"weighted\")\n",
    "f1_scr_macro = f1_score(y_test,predictions, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "write_performance_to_file(output_file, \"(B) TOP-DT\", confusion_matrix(y_test, predictions), classification_report(y_test, predictions), accuracy, f1_scr_macro, f1_scr_weighted)\n",
    "# (A) a clear separator (a sequence of hyphens or stars) and a string clearly describing the model \n",
    "    # (e.g. the model name + hyper-parameter values that you changed). In the case of Top-DT and Top-MLP,\n",
    "    # display the best hyperparameters found by the gridsearch.\n",
    "\n",
    "# (B) the confusion matrix\n",
    "\n",
    "# (C) the precision, recall, and F1-measure for each class\n",
    "\n",
    "# (D) the accuracy, macro-average F1 and weighted-average F1 of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE MLP\n",
    "(c) Base-MLP: a Multi-Layered Perceptron with 2 hidden layers of 100+100 neurons, sigmoid/logistic\n",
    "as activation function, stochastic gradient descent, and default values for the rest of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Training base MLP\n",
    "base_MLP_d2 = MLPClassifier(hidden_layer_sizes=(100,100), activation='logistic', solver='sgd')\n",
    "base_MLP_d2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Testing base MLP\n",
    "predictions = base_MLP_d2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Performance TODO : VERY BAD AND HAS A WARNING>>> RECHECK STUFF\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Output file\n",
    "f1_scr_weighted = f1_score(y_test,predictions, average=\"weighted\")\n",
    "f1_scr_macro = f1_score(y_test,predictions, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "write_performance_to_file(output_file, \"(C) Base-MLP\", confusion_matrix(y_test, predictions2), classification_report(y_test, predictions2), accuracy, f1_scr_macro, f1_scr_weighted)\n",
    "# (A) a clear separator (a sequence of hyphens or stars) and a string clearly describing the model \n",
    "    # (e.g. the model name + hyper-parameter values that you changed). In the case of Top-DT and Top-MLP,\n",
    "    # display the best hyperparameters found by the gridsearch.\n",
    "\n",
    "# (B) the confusion matrix\n",
    "\n",
    "# (C) the precision, recall, and F1-measure for each class\n",
    "\n",
    "# (D) the accuracy, macro-average F1 and weighted-average F1 of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOP MLP\n",
    "\n",
    "(d) Top-MLP: a better performing Multi-Layered Perceptron found using grid search. For this, you need\n",
    "to experiment with the following hyper-parameter values:\n",
    "• activation function: sigmoid, tanh and relu\n",
    "• 2 network architectures of your choice: for eg 2 hidden layers with 30 + 50 nodes, 3 hidden layers\n",
    "with 10 + 10 + 10\n",
    "• solver: adam and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Training top MLP\n",
    "parameters = {'activation': ['sigmoid', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(25, 50), (10, 10, 10)],\n",
    "    'solver': ['adam', 'sgd']}\n",
    "\n",
    "\n",
    "top_MLP_d2 = MLPClassifier(hidden_layer_sizes=(100,100), activation='logistic', solver='sgd')\n",
    "top_MLP_d2 = GridSearchCV(top_MLP_d2, parameters)\n",
    "\n",
    "top_MLP_d2.fit(X_train, y_train)\n",
    "\n",
    "print(top_MLP_d2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-11T06:43:47.945599800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing base MLP\n",
    "predictions = top_MLP_d2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-11T06:43:47.960726300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "print(confusion_matrix(y_test, predictions2))\n",
    "\n",
    "print(classification_report(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-11T06:43:47.992279800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output file\n",
    "f1_scr_weighted = f1_score(y_test,predictions, average=\"weighted\")\n",
    "f1_scr_macro = f1_score(y_test,predictions, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "write_performance_to_file(output_file, \"(D) TOP-MLP\", confusion_matrix(y_test, predictions2), classification_report(y_test, predictions2), accuracy, f1_scr_macro, f1_scr_weighted)\n",
    "\n",
    "# (A) a clear separator (a sequence of hyphens or stars) and a string clearly describing the model \n",
    "    # (e.g. the model name + hyper-parameter values that you changed). In the case of Top-DT and Top-MLP,\n",
    "    # display the best hyperparameters found by the gridsearch.\n",
    "\n",
    "# (B) the confusion matrix\n",
    "\n",
    "# (C) the precision, recall, and F1-measure for each class\n",
    "\n",
    "# (D) the accuracy, macro-average F1 and weighted-average F1 of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Redo steps 4 & 5, 5 times for each model and append in the performance files:\n",
    "(A) the average accuracy & the variance,\n",
    "(B) average macro-average F1 & the variance,\n",
    "(C) average weighted-average F1 & the variance.\n",
    "Does the same model give you the same performance every time? is the standard deviation high? Be ready\n",
    "to discuss this at the demo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_file"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-11T06:43:48.041820900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
